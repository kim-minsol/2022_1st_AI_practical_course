{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3주차(금)_CNN실습(공유).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "41f3f89cf6b64a4999170629806b8696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2a09cff1aba34ff391cac59e965d9884",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_42354023aa994901934dc545bcf8c2ce",
              "IPY_MODEL_6f93ecc87324427d83d02ede21dbdf4a",
              "IPY_MODEL_e232dad9345e47a1b1311c8dbaa0fd5e"
            ]
          }
        },
        "2a09cff1aba34ff391cac59e965d9884": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "42354023aa994901934dc545bcf8c2ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3a564db706d443ec93a62304bf3786b1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4fe0a5754f794b39973995d0a3e217b8"
          }
        },
        "6f93ecc87324427d83d02ede21dbdf4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a9bebdb1d8724e1792d2ad1f21006f17",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102530333,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102530333,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_48a986aac1a74c6ca2cba0017b26fc81"
          }
        },
        "e232dad9345e47a1b1311c8dbaa0fd5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_907f4fc2c3ae4512b2d43938b8236a12",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 145MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f8aea3d28c0d40daa882be847ba7e25a"
          }
        },
        "3a564db706d443ec93a62304bf3786b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4fe0a5754f794b39973995d0a3e217b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a9bebdb1d8724e1792d2ad1f21006f17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "48a986aac1a74c6ca2cba0017b26fc81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "907f4fc2c3ae4512b2d43938b8236a12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f8aea3d28c0d40daa882be847ba7e25a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kim-minsol/2022_1st_AI_practical_course/blob/main/3%EC%A3%BC%EC%B0%A8/3%EC%A3%BC%EC%B0%A8(%EA%B8%88)_CNN%EC%8B%A4%EC%8A%B5(%EA%B3%B5%EC%9C%A0).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. 데이터 불러오기\n",
        "https://drive.google.com/file/d/1M8KwdmGm8EWCn_IEWAcctbUJBww-M3cF/view?usp=sharing\n",
        "\n",
        "1. 위 링크에 있는 zip 파일을 '드라이브에 바로가기 추가'하기(안되면 그냥 다운로드 후 내 드라이브에 업로드)\n",
        "2. GPU 설정 후, 드라이브 마운트\n",
        "3. zip 파일 풀기 (약 2분 소요)"
      ],
      "metadata": {
        "id": "TDekbT7bHvKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rD68wg6GRaeQ",
        "outputId": "a835e42a-6d18-4974-9613-ce58ee7ae16f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip -uq “압축을 풀 zip 파일의 경로” -d “압축을 풀고자 하는 폴더의 경로”\n",
        "!unzip -uq /content/drive/MyDrive/plant-leaf-dataset.zip -d /content/drive/MyDrive/plant-leaf-dataset"
      ],
      "metadata": {
        "id": "0CrELDhBI3yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAmOFLpdtXV5"
      },
      "source": [
        "### 1. 데이터 분할을 위한 디렉토리 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH7lRtSlpG7c"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "original_dataset_dir = '/content/drive/MyDrive/plant-leaf-dataset/plant-leaf-dataset' #데이터셋이 위치한 경로 지정  \n",
        "classes_list = os.listdir(original_dataset_dir) #해당 경로 하위에 있는 모든 폴더의 목록을 가져옴(폴더 목록 == 클래스 목록)\n",
        " \n",
        "base_dir = '/content/drive/MyDrive/plant-leaf-dataset/plant-leaf-new-dataset' #train/val/test로 분할한 데이터를 저장할 폴더 생성\n",
        "os.mkdir(base_dir)\n",
        " \n",
        "train_dir = os.path.join(base_dir, 'train') #train 폴더 생성\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, 'val') #\bvalidation 폴더 생성\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir, 'test') #test 폴더 생성\n",
        "os.mkdir(test_dir)\n",
        "\n",
        "for cls in classes_list: #train/val/test 폴더에 각각 클래스 목록 폴더를 생성    \n",
        "    os.mkdir(os.path.join(train_dir, cls))\n",
        "    os.mkdir(os.path.join(validation_dir, cls))\n",
        "    os.mkdir(os.path.join(test_dir, cls))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. train/validation/test 데이터 분할 및 클래스 별 데이터 수 확인"
      ],
      "metadata": {
        "id": "eKJ1QY2e28i4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v0a0PUSrdnZ",
        "outputId": "de9a12b8-898d-490f-d6c7-f673b094921f"
      },
      "source": [
        "import math\n",
        "for cls in classes_list: #모든 클래스에 대한 작업 반복\n",
        "    path = os.path.join(original_dataset_dir, cls) \n",
        "    fnames = os.listdir(path) #path 위치에 존재하는 모든 이미지 파일의 목록을 fnames에 저장\n",
        "    \n",
        "    #train/validation/test 의 비율을 6:2:2로 (데이터 규모에 따라 조정 가능)\n",
        "    train_size = math.floor(len(fnames) * 0.6)\n",
        "    validation_size = math.floor(len(fnames) * 0.2)\n",
        "    test_size = math.floor(len(fnames) * 0.2)\n",
        "    \n",
        "    #train\n",
        "    train_fnames = fnames[:train_size] #train 데이터에 해당하는 파일의 이름을 train_fnames에 저장\n",
        "    for fname in train_fnames: #train 데이터에 대해 for문의 내용 반복\n",
        "        src = os.path.join(path, fname) #복사할 원본 파일의 경로 지정\n",
        "        dst = os.path.join(os.path.join(train_dir, cls), fname) #복사한 후 저장할 파일의 경로 지정\n",
        "        shutil.copyfile(src, dst) #src의 경로에 해당하는 파일을 dst의 경로에 지정\n",
        "    \n",
        "    #validation\n",
        "    validation_fnames = fnames[train_size:(validation_size + train_size)]\n",
        "    for fname in validation_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(validation_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "        \n",
        "    #test    \n",
        "    test_fnames = fnames[(train_size + validation_size):(test_size + validation_size + train_size)]\n",
        "    for fname in test_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(test_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "\n",
        "    print(\"class(\",cls,\") Train:\",len(train_fnames), \"Validation:\",len(validation_fnames), \"Test:\",len(test_fnames))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class( Apple___healthy ) Train: 987 Validation: 329 Test: 329\n",
            "class( Grape___healthy ) Train: 253 Validation: 84 Test: 84\n",
            "class( Grape___Black_rot ) Train: 708 Validation: 236 Test: 236\n",
            "class( Peach___Bacterial_spot ) Train: 1378 Validation: 459 Test: 459\n",
            "class( Potato___healthy ) Train: 91 Validation: 30 Test: 30\n",
            "class( Potato___Early_blight ) Train: 600 Validation: 200 Test: 200\n",
            "class( Corn___Common_rust ) Train: 715 Validation: 238 Test: 238\n",
            "class( Strawberry___Leaf_scorch ) Train: 671 Validation: 223 Test: 223\n",
            "class( Apple___Apple_scab ) Train: 378 Validation: 126 Test: 126\n",
            "class( Strawberry___healthy ) Train: 273 Validation: 91 Test: 91\n",
            "class( Peach___healthy ) Train: 216 Validation: 72 Test: 72\n",
            "class( Corn___healthy ) Train: 697 Validation: 232 Test: 232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYCY0sqFso7L"
      },
      "source": [
        "### 3. 기본 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucURIVBmsnmC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d6529d9-373d-4e7d-d19d-aa5eaaf20e67"
      },
      "source": [
        "import torch\n",
        "import os\n",
        " \n",
        "USE_CUDA = torch.cuda.is_available() #GPU 사용 가능한지 확인하는 메서드(사용할 수 있으면 TRUE, 없으면 FALSE 반환)\n",
        "print(USE_CUDA)\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\") #DEVICE 변수에 TRUE 이면 cuda를 FALSE 이면 cpu를 저장\n",
        "print(DEVICE)\n",
        "\n",
        "BATCH_SIZE = 512 #배치사이즈 지정\n",
        "EPOCH = 5 #에포크 지정\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "data_transforms = { # transforms.Compose()는 이미지 전처리, Augmentation 등 사용, Augmentation이란? 좌우 반전, 밝기 조절, 이미지 확대 등 노이즈를 주어 더 강한 모델을 만들어 주는 기법\n",
        "    'train': transforms.Compose([transforms.Resize([64,64]), # Resize -> 이미지의 크기를 64x64로 조정                    \n",
        "                                 transforms.RandomHorizontalFlip(), #RandomHorizontalFlip -> 이미지를 무작위로 좌우 반전\n",
        "                                 transforms.RandomVerticalFlip(), #RandomVerticalFlip -> 이미지를 무작위로 상하 반전\n",
        "                                 transforms.RandomCrop(52), #RandomCrop -> 이미지의 일부를 랜덤하게 잘라서 52x52 사이즈로 변경\n",
        "                                 transforms.ToTensor(), # ToTensor -> 이미지를 텐서 형태로 변환하고, 모든 값을 0~1 사이로 변경\n",
        "                                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), #Normalize ->정규화를 위해선 평균값과 표준편차값이 필요\n",
        "                                                                                                        #            첫번째[]는 R,G,B 채널 값에서 정규화를 적용할 평균값 \n",
        "                                                                                                        #            두번째[]는 R,G,B 채널 값에서 정규화를 적용할 표준편차값 \n",
        "                                                                                                        #            이 값은 이미지넷 데이터의 값이고, 정규화는 Local Minimum에 빠지는 것을 방지\n",
        "    'val': transforms.Compose([transforms.Resize([64,64]), \n",
        "                               #validation data는 Augmentation에 해당하는 부분을 제외하고 동일하게 전처리 \n",
        "                               transforms.RandomCrop(52), \n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 데이터 로더"
      ],
      "metadata": {
        "id": "e0zmtPpS9oAW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STs5oRi2sy12",
        "outputId": "9c199ee2-81a5-465e-8c79-ff7ee96207a8"
      },
      "source": [
        "from torchvision.datasets import ImageFolder #이미지 데이터는 하나의 클래스가 하나의 폴더에 대응되기 때문에 데이터셋을 불러올 때 ImageFolder를 사용\n",
        "\n",
        "# ImageFolder로 데이터셋 불러오기 -> root : 데이터 불러 올 경로 설정, transform : 앞서 설정한 전처리 방법 지정(불러오기 편하게 딕셔너리 형태로 구성)\n",
        "image_datasets = {x: ImageFolder(root=os.path.join(base_dir, x), transform=data_transforms[x]) for x in ['train', 'val']} \n",
        "\n",
        "# DataLoader로 불러온 이미지 데이터를 주어진 조건에 따라 미니 배치 단위로 분리 -> shuffle=True : 데이터의 순서가 섞여 학습시에 Label 정보의 순서를 기억하는 것을 방지 할 수 있음 필수!\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for x in ['train', 'val']} \n",
        "\n",
        "#train/validation의 총 개수를 저장\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "#12개 클래스의 목록을 저장\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "print(class_names)\n",
        "print(len(class_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Apple___Apple_scab', 'Apple___healthy', 'Corn___Common_rust', 'Corn___healthy', 'Grape___Black_rot', 'Grape___healthy', 'Peach___Bacterial_spot', 'Peach___healthy', 'Potato___Early_blight', 'Potato___healthy', 'Strawberry___Leaf_scorch', 'Strawberry___healthy']\n",
            "12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. 전이학습 모델 불러오기\n",
        "1. 모델만 불러와서 구조 print 해보기\n",
        "2. 분류층 바꾸고 print 해보기"
      ],
      "metadata": {
        "id": "Uy5j3kc79q6x"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZEFZgmTs2Vt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "41f3f89cf6b64a4999170629806b8696",
            "2a09cff1aba34ff391cac59e965d9884",
            "42354023aa994901934dc545bcf8c2ce",
            "6f93ecc87324427d83d02ede21dbdf4a",
            "e232dad9345e47a1b1311c8dbaa0fd5e",
            "3a564db706d443ec93a62304bf3786b1",
            "4fe0a5754f794b39973995d0a3e217b8",
            "a9bebdb1d8724e1792d2ad1f21006f17",
            "48a986aac1a74c6ca2cba0017b26fc81",
            "907f4fc2c3ae4512b2d43938b8236a12",
            "f8aea3d28c0d40daa882be847ba7e25a"
          ]
        },
        "outputId": "81d1ee2b-99e7-4ab3-98bd-86280b1354d8"
      },
      "source": [
        "from torchvision import models #pytorch 공식문서에서 확인 한 것처럼, 여기서 여러 모델을 불러올 수 있음\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#resnet18/34/50 \n",
        "model = models.resnet50(pretrained=True) #pretrained=True로 설정하면 pre-trained model의 parameter값을 그대로 가져옴, False로 설정하면 모델의 아키텍처만 가져오고 parameter는 랜덤 설정\n",
        "num_ftrs = model.fc.in_features #모델의 마지막 레이어의 입력 채널의 수를 저장(in_features는 해당 레이어의 입력 채널 수를 의미)   \n",
        "model.fc = nn.Linear(num_ftrs, len(class_names)) #모델의 마지막 레이어를 새로운 레이어로 교체 (입력 채널 수는 기존 레이어와 동일, 출력 채널 수를 우리가 원하는 수로 설정하는 것! 여기서는 클래스 수 12개) \n",
        "\n",
        "'''\n",
        "#vgg16/19\n",
        "model = models.vgg16(pretrained=True)\n",
        "#model.classifier[6].out_features = len(class_names) #마지막 레이어를 교체하는 방법이 약간 다름, print 해서 구조 확인하면서 이해\n",
        "\n",
        "#mobilenet_v2\n",
        "model = models.mobilenet_v2(pretrained=True)\n",
        "#model.classifier[1].out_features = len(class_names)\n",
        "\n",
        "#mobilnet_v3_small\n",
        "model = models.mobilenet_v3_small(pretrained=True)\n",
        "#model.classifier[3].out_features = len(class_names)\n",
        "'''\n",
        "\n",
        "model = model.to(DEVICE) #모델 gpu에 태우기\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41f3f89cf6b64a4999170629806b8696",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=12, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Layer Freeze"
      ],
      "metadata": {
        "id": "4zzyFflRf13T"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wf8IIPgs3vs"
      },
      "source": [
        "cnt = 0 #몇 번째 Layer인지 나타내는 변수 cnt 설정\n",
        "for child in model.children(): #모델의 모든 Layer 정보를 담고 있음 (vgg, mobilenet 계열은 model.features)\n",
        "    cnt += 1 \n",
        "    #import pdb;pdb.set_trace() #디버거 cnt,n,c,child,q\n",
        "    if cnt < 8: #resnet50기준 10개의 Layer중 1~5개는 Freeze하고, 6~10은 학습 시 parameter를 업데이트 하도록!\n",
        "        #print(child)\n",
        "        for param in child.parameters(): #vgg, mobilenet 계열은 model.features.parameters()\n",
        "            param.requires_grad = False  #False -> NO UPDATE(FREEZE), True -> UPDATE(기본값)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. 손실함수, 최적화함수, 스케쥴러 설정\n",
        "- Adam vs SGD\n",
        "- learning rate는 작게!\n",
        "- 미리 학습 코드까지 실행!"
      ],
      "metadata": {
        "id": "onKCFqbZf9oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 학습에 사용하는 Loss 함수를 지정\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Optimizer는 Adam, filter와 lambda를 사용하는 이유 : param.requires_grad = True로 설정된 Layer의 parameter만을 업데이트 하기 위해서!\n",
        "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.00001) #전이학습 시에는 lr 낮게(과적합 방지)\n",
        " \n",
        "from torch.optim import lr_scheduler\n",
        "# 에포크에 따라 Learning Rate를 변경하는 역할 (7 에포크마다 0.1씩 곱해 LR을 감소시킴), Why? : 학습 보폭을 정하는 일은 매우 중요한데, 처음엔 크게 -> 학습 진행될 수록 작게 설정하는 것이 좋다고 알려짐, but 아직 연구중\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "LfwDUXcaD_uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. 모델 학습 및 저장"
      ],
      "metadata": {
        "id": "t86IqtKnK8Qr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXFjVMs3s5Jv"
      },
      "source": [
        "# 전이학습 모델 학습 및 검증\n",
        "import time\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    \n",
        "    train_losses , train_accuracy = [],[] #그래프 그리기 위해서 train에 대한 loss,accuracy 저장\n",
        "    val_losses , val_accuracy = [],[] #그래프 그리기 위해서 validation에 대한 loss,accuracy 저장\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())  #정확도가 가장 높은 모델을 저장\n",
        "    best_acc = 0.0 #정확도가 가장 높은 모델의 정확도 저장\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print('-------------- epoch {} ----------------'.format(epoch+1)) \n",
        "        since = time.time() #한 에포크 돌 때 소요되는 시간 측정(시작 시각 저장)                                    \n",
        "        for phase in ['train', 'val']: #한 에포크 돌 때 train 한 번, validation 한 번씩 각각 진행\n",
        "            if phase == 'train': \n",
        "                model.train() #train이면 학습 모드\n",
        "            else:\n",
        "                model.eval() #validation이면 평가 모드(평가 때 사용하지 말아야 할 작업들 알아서 꺼줌, dropout이나 batchnorm layer 같은 것들)     \n",
        " \n",
        "            running_loss = 0.0   #모든 데이터의 loss를 합해서 저장\n",
        "            running_corrects = 0 #정확하게 예측한 경우의 수를 저장\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]: #모델의 현재 모드(train or validation)에 해당하는 Dataloader에서 데이터를 받는 for문\n",
        "                inputs = inputs.to(DEVICE) #데이터를 gpu에 태움 \n",
        "                labels = labels.to(DEVICE) #데이터의 라벨값을 gpu에 태움\n",
        "                \n",
        "                optimizer.zero_grad() #학습 진행하면 이전 Batch의 Gradient값이 Optimizer에 저장될 것이므로 초기화 해주고 시작해야 함\n",
        "                \n",
        "                with torch.set_grad_enabled(phase == 'train'): #set_grad_enabled를 이용하면 train 모드에서만 모델의 Gradient를 업데이트 하도록 설정 할 수 있음\n",
        "                    outputs = model(inputs) #드디어 데이터를 모델에 입력!\n",
        "                    _, preds = torch.max(outputs, 1) #모델에 입력된 데이터가 12개의 클래스에 속할 확률값 출력, 이 중 가장 높은 값의 인덱스를 예측값으로 preds에 저장\n",
        "                    loss = criterion(outputs, labels) #모델의 예측값과 정답값 사이의 Loss를 계산(criterion 함수는 위에서 미리 설정해 둔 것)\n",
        "    \n",
        "                    if phase == 'train':   \n",
        "                        loss.backward() #계산한 loss값을 이용하여 BackPropagation을 통해 계산한 Gradient값을 parameter에 할당하고,\n",
        "                        optimizer.step() #모델의 parameter 업데이트\n",
        " \n",
        "                running_loss += loss.item() * inputs.size(0) #모든 데이터의 loss를 합해서 저장하기 위해, 하나의 미니 배치에 대한 loss값에 데이터의 수를 곱해서 더함 (inputs.size(0)이 미니 배치의 수) \n",
        "                running_corrects += torch.sum(preds == labels.data) #예측값과 정답값이 같으면 증가!\n",
        "\n",
        "            if phase == 'train':  \n",
        "                scheduler.step() #위에서 미리 설정한 Scheduler 실행\n",
        " \n",
        "            epoch_loss = running_loss/dataset_sizes[phase] #해당 에포크의 loss를 계산하기 위해 running_loss를 데이터셋 사이즈로 나눔\n",
        "            epoch_acc = running_corrects.double()/dataset_sizes[phase] #정확도도 마찬가지로 running_corrects를 데이터셋 사이즈로 나눔\n",
        " \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc)) #해당 에포크의 loss와 정확도를 매번 출력\n",
        "\n",
        "            if phase == 'train': #그래프 그리기 위해 train 데이터의 loss와 accuracy 따로 저장\n",
        "                train_losses.append(epoch_loss)\n",
        "                train_accuracy.append(epoch_acc)\n",
        "            if phase == 'val': #그래프 그리기 위해 \bvalidation 데이터의 loss와 accuracy 따로 저장\n",
        "                val_losses.append(epoch_loss)\n",
        "                val_accuracy.append(epoch_acc)\n",
        "          \n",
        "            if phase == 'val' and epoch_acc > best_acc: #validation 모드에서 정확도가 최고 정확도 보다 높으면 업데이트\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict()) #최고 정확도를 가진 모델을 best_model_wts 변수에 저장\n",
        " \n",
        "        time_elapsed = time.time() - since #한 에포크 돌 때 소요되는 시간 측정(종료 시각 - 시작 시각) \n",
        "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) #계산한 시간 분과 초로 출력\n",
        "\n",
        "    #학습 종료 후 \n",
        "    print('Best validation Acc: {:4f}'.format(best_acc)) #validation 중 최고 정확도 출력\n",
        "\n",
        "    #train과 validation의 loss, accuracy 그래프 출력 -> 과적합 여부 등 판단\n",
        "    plt.plot(range(1,len(train_losses)+1),train_losses,'bo',label = 'training loss')\n",
        "    plt.plot(range(1,len(val_losses)+1),val_losses,'r',label = 'validation loss')\n",
        "    plt.legend()\n",
        "    plt.plot(range(1,len(train_accuracy)+1),train_accuracy,'co',label = 'training accuracy')\n",
        "    plt.plot(range(1,len(val_accuracy)+1),val_accuracy,'m',label = 'validation accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    #정확도가 가장 높았던 모델을 불러와서 반환\n",
        "    model.load_state_dict(best_model_wts) \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        },
        "id": "EQ6wBtMAs6pw",
        "outputId": "84b64772-3c24-4b6b-dfa5-8cd685f03b99"
      },
      "source": [
        "# 전이학습 실행\n",
        "model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=EPOCH) \n",
        "\n",
        "# 반환 받은 정확도가 가장 높았던 모델을 torch.save 이용해서 저장 (모델 별로 이름 변경해서 저장!)\n",
        "torch.save(model, '/content/drive/MyDrive/plant-leaf-dataset/resnet50.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------- epoch 1 ----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.5160 Acc: 0.8916\n",
            "val Loss: 0.4552 Acc: 0.9047\n",
            "Completed in 0m 23s\n",
            "-------------- epoch 2 ----------------\n",
            "train Loss: 0.3901 Acc: 0.9178\n",
            "val Loss: 0.3566 Acc: 0.9198\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 3 ----------------\n",
            "train Loss: 0.3076 Acc: 0.9381\n",
            "val Loss: 0.2943 Acc: 0.9319\n",
            "Completed in 0m 23s\n",
            "-------------- epoch 4 ----------------\n",
            "train Loss: 0.2454 Acc: 0.9473\n",
            "val Loss: 0.2271 Acc: 0.9513\n",
            "Completed in 0m 24s\n",
            "-------------- epoch 5 ----------------\n",
            "train Loss: 0.2075 Acc: 0.9541\n",
            "val Loss: 0.1991 Acc: 0.9504\n",
            "Completed in 0m 23s\n",
            "Best validation Acc: 0.951293\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjkyMbUEuMqi"
      },
      "source": [
        "### 7. 모델 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPwYDjEHtCXm",
        "outputId": "02893f19-fa71-4934-b448-360419bf86f8"
      },
      "source": [
        "# 전이학습 평가 전처리 (위에서 설명한 것과 동일)\n",
        "data_transforms = transforms.Compose([ \n",
        "        transforms.Resize([64,64]),  \n",
        "        transforms.RandomCrop(52),  \n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
        "        ])\n",
        "\n",
        "#경로 맞춰서 변경해 주세요!\n",
        "test_dataset = ImageFolder(root='/content/drive/MyDrive/plant-leaf-dataset/plant-leaf-new-dataset/test', transform=data_transforms) \n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "# 모델 평가 함수\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval() #모델을 평가 모드로 설정\n",
        "    test_loss = 0 #미니 배치 별로 loss를 합산해서 저장\n",
        "    correct = 0 #정확하게 예측한 수 저장   \n",
        "    with torch.no_grad(): #해당 메서드를 이용해서 parameter 업데이트 방지\n",
        "        for data, target in test_loader:  \n",
        "            data, target = data.to(DEVICE), target.to(DEVICE) #데이터와 라벨을 불러오면서 gpu에 태움  \n",
        "            output = model(data) #데이터를 모델에 입력           \n",
        "            test_loss += torch.nn.functional.cross_entropy(output,target, reduction='sum').item() #모델의 예측값과 정답값 사이의 loss 계산\n",
        "            pred = output.max(1, keepdim=True)[1]  #모델에 입력된 데이터가 12개의 클래스에 속할 확률값 출력, 이 중 가장 높은 값의 인덱스를 예측값으로 pred에 저장\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item() #target.view_as(pred)를 이용해 target의 텐서 구조를 pred의 텐서와 같은 모양으로 재정렬 (모델 만들 때 쓰는 view와 비슷 view는 숫자 직접 지정)\n",
        "                                                                  #eq는 비교 연산자로 pred와 target.view_as(pred)의 값이 일치하면 1, 일치하지 않으면 0 반환\n",
        "   \n",
        "    test_loss /= len(test_loader.dataset) #모든 미니 배치에서 합한 loss값을 배치 수로 나누어 loss값의 평균 구함\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset) #마찬가지로 정확도의 평균도 구함\n",
        "    \n",
        "    return test_loss, test_accuracy #계산한 Test 데이터의 loss와 정확도 반환\n",
        "\n",
        "# 전이학습 모델 평가 결과\n",
        "model=torch.load('/content/drive/MyDrive/plant-leaf-dataset/resnet50.pt') #torch.load를 이용해서 원하는 모델 불러오기!\n",
        "test_loss, test_accuracy = evaluate(model, test_loader) #평가 함수 이용해서 Test 데이터에 대한 loss 및 정확도 측정\n",
        "print('model test acc:  ', test_accuracy) #평가 정확도 출력"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model test acc:   95.25862068965517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(과제) 한 가지 이상의 변화를 준 후 학습을 돌려서 결과와 함께 간단한 설명을 업로드 해주세요 😀\n",
        "\n",
        "예시 : 다른 전이학습 모델 사용, freeze 시키는 구간 변화, 직접 짠 모델과의 성능 비교, 데이터 수의 변화, optimizer에 대한 실험, epoch 늘리기, 등등"
      ],
      "metadata": {
        "id": "maEk9ITatoai"
      }
    }
  ]
}